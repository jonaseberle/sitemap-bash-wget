#!/bin/bash
# - scrapes URLs starting with a given URL and writes a sitemap.xml
# - keeps temp files 
# - based on @link http://www.lostsaloon.com/technology/how-to-create-an-xml-sitemap-using-wget-and-shell-script/
# @author jonas.eberle@d-mind.de

#set -x

if [ -z "$2" ]; then
	echo Usage: "$0 ‹domain URL› ‹sitemap.xml location› ‹further wget params, e.g. --domains=aaa.de,aaa.com -H›"
	exit 1
fi

startUrl="$1"
shift
sitemapFilePath="$1"
shift

urlsFilePath="${sitemapFilePath}.tmp.txt"
sortedUrlsFilePath="${sitemapFilePath}.txt"

echo "Logs: $urlsFilePath"

wget --no-clobber --recursive --follow-tags=a,form --adjust-extension --level=inf --no-verbose --output-file="$urlsFilePath" $startUrl $@
grep -i URL "$urlsFilePath" | awk -F 'URL:' '{print $2}' | awk '{$1=$1};1' | awk '{print $1}' | sort -u | sed '/^$/d' > "$sortedUrlsFilePath"
header='<?xml version="1.0" encoding="UTF-8"?>
    <urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	    xsi:schemaLocation="http://www.sitemaps.org/schemas/sitemap/0.9/sitemap.xsd">' 
echo $header > "$sitemapFilePath"
while read p; do
	case "$p" in 
		*/ | *.html | *.htm)
		echo '<url><changefreq>daily</changefreq><loc>'$p'</loc></url>' >> "$sitemapFilePath"
		;;  
	*)
		;;
	esac
done < "$sortedUrlsFilePath"
echo "</urlset>" >> "$sitemapFilePath"


