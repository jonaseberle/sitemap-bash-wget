#!/bin/bash
# - scrapes URLs starting with a given URL and writes a sitemap.xml
# - keeps temp files 
# - based on @link http://www.lostsaloon.com/technology/how-to-create-an-xml-sitemap-using-wget-and-shell-script/
# @author jonas.eberle@d-mind.de

#set -x

if [ -z "$2" ]; then
	echo Usage: "$0 ‹domain URL› ‹sitemap.xml location› ‹further wget params, e.g. --domains=aaa.de,aaa.com -H›"
	exit 1
fi

startUrl="$1"
shift
sitemapFilePath="$1"
shift

urlsFilePath="${sitemapFilePath}.tmp.txt"
sortedUrlsFilePath="${sitemapFilePath}.txt"

echo "Logs: $urlsFilePath"

wget --no-clobber --recursive --follow-tags=a,form --adjust-extension --no-verbose --output-file="$urlsFilePath" $startUrl $@
grep -i URL "$urlsFilePath" | awk -F 'URL:' '{print $2}' | awk '{$1=$1};1' | awk '{print $1}' | sort -u | sed '/^$/d' > "$sortedUrlsFilePath"

path=$(readlink -f $(dirname "$0") )

$(dirname $(readlink -f "$0") )/sitemapTxt2Xml "$sortedUrlsFilePath" "$sitemapFilePath"
